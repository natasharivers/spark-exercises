{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who? What? When? Where? Why? How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?\n",
    "   - framework/ platform for dealing with big data\n",
    "\n",
    "#### 4 V's of Big Data    \n",
    "   - **Velocity**: \n",
    "        - data coming in quickly \n",
    "        - possibly streaming\n",
    "   - **Volume**: \n",
    "        - a lot of data\n",
    "        - possibly doesn't fit in memory or storage\n",
    "   - **Variety**: \n",
    "        - a lot of sources \n",
    "        - different formats \n",
    "        - might not be clean\n",
    "   - **Veracity**: \n",
    "        - AKA reliability\n",
    "        - might have missing data\n",
    "        \n",
    "- Spark Steaming, - Databricks\n",
    "\n",
    "## When do we use it?\n",
    "   - We use Spark when data is too big for memory or storage\n",
    "        - memory: RAM, loaded up and ready to go\n",
    "        - storage: Harddrive\n",
    "        \n",
    "## Where is it used?        \n",
    "   - Spark is used when it's already set up\n",
    "       - if its already set up in your environment, you'll use it\n",
    "  \n",
    "## How do we use it?\n",
    "   - We use Spark by accessing client libraries\n",
    "  \n",
    "## Alternatives:\n",
    "   - hadoop\n",
    "   - dask\n",
    "   **Spark is the most popular**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Architecture\n",
    "\n",
    "- Scala on the JVM\n",
    "    - Scala is a programing language that runs on Java Virtual Machine (JVM)\n",
    "    - If you can run Java, you can run Spark\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Client Libraries that talk to the running spark instance\n",
    "    - pyspark\n",
    "    - sparkR\n",
    "    - Spark SQL: write SQL queries against Spark data\n",
    "    **all ends up as the same Spark code**\n",
    "    \n",
    "<br>\n",
    "\n",
    "- Computers in a Spark Cluster\n",
    "    - Driver: your laptop\n",
    "    - Cluster Manager/Master: machine that organizes everything (computer in the cloud)\n",
    "    - Executors: spark machines that \"do the work\" under the direction of the Manager (computer in the cloud)\n",
    "\n",
    "<br>\n",
    "\n",
    "- Spark Local Mode:\n",
    "    - still uses all the pieces of Spark\n",
    "    - single executor\n",
    "    - all self contained in your laptop\n",
    "    - less common in practice\n",
    "        - can be useful for data that fits in stoarge but not in memory\n",
    "    \n",
    "#### Spark code for a full cluster and local mode is exactly the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel Work & Spark Dataframes\n",
    "\n",
    "- Spark does work in parallel (multiple things happen at once)\n",
    "<br>\n",
    "\n",
    "- faster at scale, but some upfront overhead\n",
    "    - as the size of the data gets bigger, the approach can be fast BUT theres a computational cost to set up\n",
    "        - ex: scape READMEs of 1M repos\n",
    "            - this will take a WHILE on one latop\n",
    "            - BUT if you split it among several classmates then put it the parts together... it'll be faster     \n",
    "        - ex: scape READMEs of 20 repos\n",
    "            - this will be faster on one latop\n",
    "            - spliting one per person then putting it together.. will take too long\n",
    "            \n",
    "<br>\n",
    "\n",
    "- **Executor**: \n",
    "    - any computer with access to the spark\n",
    "- **Partitions**:\n",
    "    - stores data in seperate data partitians\n",
    "    - number of partitians is equal to the number of CPU cores it have\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Spark Dataframes**:\n",
    "    - Abstract everything above\n",
    "    - simiar to Pandas DF\n",
    "        - Diffences:\n",
    "            - Spark is Lazy!\n",
    "\n",
    "<br>\n",
    "\n",
    "- **Two operations** within Spark:\n",
    "    - **Actions**: starts a job\n",
    "        - This is the first step\n",
    "        - ex: show me data in the dataframe\n",
    "        \n",
    "    - **Transformations**: changes the thing\n",
    "        - are lazy\n",
    "        - ex: add one to all the columns, get average of all columns, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
